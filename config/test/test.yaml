env_params:
  n_player: 2
  # env_name: "football-1_vs_1_easy"
  env_name: "football-11_vs_11_stochastic"
  num_envs: 2
  is_obs_continuous: false
  is_act_continuous: false
  obs_type: 
    - "dict"
    - "dict"
  max_step: 3000
  agent_nums: 
    - 1
    - 0
  act_box: 
    discrete_n: 19
policy_params:
  lr: 0.00001
  train_iters: 5
  clip_ratio: 0.2
  target_kl: 0.01
  ent_coef: 0.1
  v_coef: 0.5
  clip_grads: 0.5


  pool_size: [2, 2]
  mlp_filters: 
    - 64
    - 64
    - 64
    - 64
  activation: "relu"
  output_activation: 
  kernel_initializer: "glorot_uniform"
  input_shape: 
    - 217
  act_size: 19

train_params:
  trainer: "Proximal Policy Optimization"
  nn_architecure: "mlp_simple_actor_critic"

  epochs: 1000
  steps_per_epoch: 300
  num_mini_batches: 4
  gamma: 0.99
  lam: 0.97
  save_freq: 5

  restore_path:
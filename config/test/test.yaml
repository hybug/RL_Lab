env_params:
  n_player: 2
  # env_name: "football-1_vs_1_easy"
  env_name: "football-11_vs_11_easy_stochastic"
  num_envs: 2
  is_obs_continuous: false
  is_act_continuous: false
  obs_type: 
    - "dict"
    - "dict"
  max_step: 3000
  agent_nums: 
    - 1
    - 0
  act_box: 
    discrete_n: 19
policy_params:
  lr: 0.00011879 
  train_iters: 5
  clip_ratio: 0.2
  target_kl: 0.01
  ent_coef: 0.00155
  v_coef: 0.5
  clip_grads: 0.5


  pool_size: [1, 1]
  resnet_filters: 
    - [16, 3, 1, 2]
    - [32, 3, 1, 2]
    - [32, 3, 1, 2]
    - [32, 3, 1, 2]
  activation: "relu"
  output_activation: "relu"
  kernel_initializer: "glorot_uniform"
  input_shape: [72, 96, 4]
  act_size: 19

train_params:
  trainer: "Proximal Policy Optimization"
  nn_architecure: "impala_cnn_actor_critic"

  epochs: 1000
  steps_per_epoch: 512
  num_mini_batches: 4
  gamma: 0.997
  lam: 0.97
  save_freq: 5

  restore_path: